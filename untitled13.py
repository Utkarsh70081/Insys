# -*- coding: utf-8 -*-
"""Untitled13.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lkOCziPAOrwooNxjXMZ5dGDLMfkHYgt9
"""

pip install PyPDF2 sentence-transformers faiss-cpu pinecone-client

# Import necessary libraries
import os
from PyPDF2 import PdfReader
from sentence_transformers import SentenceTransformer
import faiss
import numpy as np
import pinecone

# Function to upload and validate the PDF file
def upload_and_validate_pdf(file_path):
    """
    Validates if the given file is a valid PDF.
    Args:
        file_path (str): Path to the PDF file.
    Returns:
        bool: True if valid, False otherwise.
    """
    if not os.path.isfile(file_path):
        raise FileNotFoundError(f"File not found: {file_path}")
    if not file_path.endswith('.pdf'):
        raise ValueError(f"Invalid file type. Expected a PDF: {file_path}")

    try:
        # Attempt to read the PDF
        PdfReader(file_path)
        return True
    except Exception as e:
        raise ValueError(f"Invalid PDF file: {e}")

# Function to extract text from the PDF
def extract_text_from_pdf(file_path):
    """
    Extracts text from a PDF file.
    Args:
        file_path (str): Path to the PDF file.
    Returns:
        str: Extracted text.
    """
    try:
        reader = PdfReader(file_path)
        text = ""
        for page in reader.pages:
            text += page.extract_text() or ""  # Append text from each page
        if not text.strip():
            raise ValueError("No text found in the PDF.")
        return text
    except Exception as e:
        raise RuntimeError(f"Error extracting text: {e}")

# Function to chunk the extracted text into smaller overlapping chunks
def chunk_text(text, chunk_size=500, overlap=50):
    """
    Splits text into smaller overlapping chunks.
    Args:
        text (str): The text to chunk.
        chunk_size (int): Size of each chunk.
        overlap (int): Overlapping size between chunks.
    Returns:
        list: List of text chunks.
    """
    words = text.split()
    chunks = []
    for i in range(0, len(words), chunk_size - overlap):
        chunk = words[i:i + chunk_size]
        chunks.append(" ".join(chunk))
    return chunks

# Function to generate embeddings for the text chunks
def generate_embeddings(chunks):
    """
    Generates embeddings for a list of text chunks.
    Args:
        chunks (list): List of text chunks.
    Returns:
        list: List of embeddings.
    """
    model = SentenceTransformer('all-MiniLM-L6-v2')
    embeddings = model.encode(chunks)
    return embeddings

# Function to create a FAISS index from the embeddings
def create_faiss_index(embeddings):
    """
    Creates a FAISS index from embeddings and stores them locally.
    Args:
        embeddings (list): List of embeddings.
    Returns:
        faiss.IndexFlatL2: FAISS index.
    """
    dimension = len(embeddings[0])
    index = faiss.IndexFlatL2(dimension)
    index.add(np.array(embeddings, dtype=np.float32))
    return index

# Function to connect to a Pinecone index
def connect_to_pinecone(index_name, api_key, environment):
    """
    Connects to a hosted Pinecone vector database.
    Args:
        index_name (str): Name of the index.
        api_key (str): Pinecone API key.
        environment (str): Pinecone environment.
    Returns:
        pinecone.Index: Connected index.
    """
    pinecone.init(api_key=api_key, environment=environment)
    if index_name not in pinecone.list_indexes():
        pinecone.create_index(index_name, dimension=384)  # Create index if it doesn't exist
    index = pinecone.Index(index_name)
    return index

# Function to upload embeddings to Pinecone
def upload_embeddings_to_pinecone(index, embeddings, ids):
    """
    Uploads embeddings to Pinecone.
    Args:
        index (pinecone.Index): Pinecone index.
        embeddings (list): List of embeddings.
        ids (list): List of IDs corresponding to the embeddings.
    """
    vectors = [(str(ids[i]), embeddings[i]) for i in range(len(embeddings))]
    index.upsert(vectors)  # Upload the vectors to Pinecone

# Example Workflow
# Step 1: Upload and validate PDF
# ////////////////////////////////////////////////////////////////////////////////////////////////////////
file_path = "example.pdf"
# ///////////////////////////////////////////////////////////////////////////////////////////////////////
if upload_and_validate_pdf(file_path):
    print("PDF validated successfully.")

# Step 2: Extract text from the PDF
text = extract_text_from_pdf(file_path)
print("Extracted text.")

# Step 3: Chunk the extracted text
chunks = chunk_text(text)
print(f"Text split into {len(chunks)} chunks.")

# Step 4: Generate embeddings for the chunks
embeddings = generate_embeddings(chunks)
print("Generated embeddings.")

# Step 5: Create a FAISS index from the embeddings
faiss_index = create_faiss_index(embeddings)
print("FAISS index created.")

# Step 6: Connect to Pinecone and upload the embeddings
pinecone_api_key = "YOUR_API_KEY"
pinecone_environment = "YOUR_ENVIRONMENT"
# /////////////////////////////////////////////////////////////////////////////////////////////////
index_name = "example-index"
# /////////////////////////////////////////////////////////////////////////////////////////////////
pinecone_index = connect_to_pinecone(index_name, pinecone_api_key, pinecone_environment)
upload_embeddings_to_pinecone(pinecone_index, embeddings, ids=range(len(embeddings)))
# print("Uploaded embeddings to Pinecone.")